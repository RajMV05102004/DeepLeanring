{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPSHQtvVW8/PRr+kv4C20gm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RajMV05102004/DeepLeanring/blob/main/WikiWordPredictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVTbCyQFUExC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the file assuming it's space-separated or tab-separated\n",
        "df = pd.read_csv(r\"/content/wiki.train.tokens\", sep=\"\\t\", header=None, names=[\"Text\"])\n",
        "\n",
        "# Display the first few lines\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df=pd.read_csv(r\"/content/wiki.valid.tokens\", sep=\"\\t\", header=None, names=[\"Text\"])"
      ],
      "metadata": {
        "id": "sQS3-4tRlTmn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_df.sample(5)"
      ],
      "metadata": {
        "id": "6qXLqfaks5tg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(df,col):\n",
        "  #Replacing unk with empty string\n",
        "  df[col] = df[col].str.replace(\"<unk>\", \"\")\n",
        "  # Remove all special characters using regex\n",
        "  df[col] = df[col].str.replace(r\"[^a-zA-Z\\s]\", \"\", regex=True)\n",
        "  #Convert everything to lowercase\n",
        "  df[col]=df[col].str.lower()\n"
      ],
      "metadata": {
        "id": "Bd91kl9Gq69u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(df,'Text')"
      ],
      "metadata": {
        "id": "aBOKuJIvtkJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(valid_df,'Text')"
      ],
      "metadata": {
        "id": "CHptnjINrVtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "6A14179jX3XE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()"
      ],
      "metadata": {
        "id": "lP6McFEDcOfO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fit_tokenizer(obj,tokenizer,col):\n",
        "  tokenizer.fit_on_texts(obj[col])\n"
      ],
      "metadata": {
        "id": "d2lttvoicRHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit_tokenizer(df,tokenizer,'Text')"
      ],
      "metadata": {
        "id": "5Ag-2K9EcsNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fit_tokenizer(valid_df,tokenizer,'Text')"
      ],
      "metadata": {
        "id": "thPRTVyatd5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_index"
      ],
      "metadata": {
        "id": "Xk5zlQ17cyAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.word_counts"
      ],
      "metadata": {
        "id": "IB-jJDOuc1Uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer.word_index)"
      ],
      "metadata": {
        "id": "vYH5nSSJeRWK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Train_data=df.copy()# Storing the original DataFrame\n",
        "Valid_data=df.copy()# Storing the Validity DataFrame"
      ],
      "metadata": {
        "id": "FfVDIyDptxpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We are creating a dataset where a sequence of words are stored in an non-decreasing manner\n",
        "def createDataset(df,tokenizer):\n",
        "  input_sequence=[]\n",
        "\n",
        "  for sentence in df[\"Text\"]:\n",
        "    tokennized_sentence=tokenizer.texts_to_sequences([sentence])[0]\n",
        "\n",
        "    for i in range(1,len(tokennized_sentence)):\n",
        "      n_gram=tokennized_sentence[:i+1]\n",
        "      input_sequence.append(n_gram)\n",
        "  return input_sequence"
      ],
      "metadata": {
        "id": "s5KJv2Tme5MZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_sequence=createDataset(df,tokenizer)"
      ],
      "metadata": {
        "id": "raZbPZjAuA-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_input_sequence=createDataset(valid_df,tokenizer)"
      ],
      "metadata": {
        "id": "jjt7jbBTry8G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_input_sequence[:]"
      ],
      "metadata": {
        "id": "XM0NXVPbsaK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(valid_input_sequence)"
      ],
      "metadata": {
        "id": "M4Vqv2YcsWi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We need the maximum length in the input sequence\n",
        "valid_maxlen=max(len(x) for x in valid_input_sequence)"
      ],
      "metadata": {
        "id": "jYg0iODMsHwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#We need the maximum length in the input sequence\n",
        "maxlen=max(len(x) for x in input_sequence)"
      ],
      "metadata": {
        "id": "4Yfx8HMLirS8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "maxlen# This is the  maximum size of the input sequence"
      ],
      "metadata": {
        "id": "2yn9TupzjKH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valid_maxlen"
      ],
      "metadata": {
        "id": "ZAmJGz8ksOL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "nkLYtWrjjLNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def padding(sequence,maxlen):\n",
        "  #now we will pad the input sequences to the maxlen\n",
        "  return pad_sequences(sequence,maxlen=maxlen,padding='pre')"
      ],
      "metadata": {
        "id": "xkFReA1WkhEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_padded_sequence=padding(input_sequence,maxlen)\n",
        "valid_padded_sequence=padding(valid_input_sequence,valid_maxlen)"
      ],
      "metadata": {
        "id": "Ba0MZVPPvPg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "sGg6awi_pC0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the processed padded sequence in a pickle file"
      ],
      "metadata": {
        "id": "l17DKBkcvgbu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input_padded_sequence.pkl', 'wb') as f:\n",
        "    pickle.dump((input_padded_sequence, tokenizer), f)"
      ],
      "metadata": {
        "id": "OkFawoG0pFaO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('valid_padded_sequence.pkl', 'wb') as f:\n",
        "    pickle.dump((valid_padded_sequence, tokenizer), f)"
      ],
      "metadata": {
        "id": "7Ptw-XmZvfLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing is done till here\n",
        "Now loading the Test and Validation sequences"
      ],
      "metadata": {
        "id": "KRGa1zzfziae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle"
      ],
      "metadata": {
        "id": "_wbEhjpSzh3O"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/input_padded_sequence.pkl', 'rb') as f:\n",
        "    input_padded_sequence, tokenizer = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "d4TYf9jxzOGG"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/valid_padded_sequence.pkl', 'rb') as f:\n",
        "    valid_padded_sequence, tokenizer = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "Exd1oXBlz2aH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=input_padded_sequence[:,:-1]\n",
        "y_train=input_padded_sequence[:,-1]\n",
        "#y=np.expand_dims(y,axis=1)\n",
        "X_val=valid_padded_sequence[:,:-1]\n",
        "y_val=valid_padded_sequence[:,-1]"
      ],
      "metadata": {
        "id": "_0-OwhtUlH8K"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "76E5_475wtFO"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Input:\n",
        "\n",
        "1. y: The labels (target values) for your dataset. These are typically integer-encoded class labels (e.g., [0, 1, 2, ...]).\n",
        "\n",
        "2. tokenizer: A tokenizer object (e.g., from Keras' Tokenizer class) that has been fitted on the text data. It contains the vocabulary and word-to-index mappings.\n",
        "\n",
        "# Purpose:\n",
        "\n",
        "The function converts the integer-encoded labels (y) into a one-hot encoded format, which is required for multiclass classification problems when using a softmax activation in the output layer.\n",
        "\n",
        "# to_categorical:\n",
        "\n",
        "1. This is a utility function from Keras (keras.utils.to_categorical) that converts a class vector (integers) into a binary class matrix (one-hot encoding).\n",
        "\n",
        "2. For example, if y = [0, 1, 2] and num_classes=3, the output will be:\n",
        "  [[1., 0., 0.],\n",
        "  [0., 1., 0.],\n",
        "  [0., 0., 1.]]\n",
        "  num_classes=len(tokenizer.word_index)+1:\n",
        "\n",
        "  len(tokenizer.word_index) gives the size of the vocabulary (number of unique words).\n",
        "\n",
        "3. +1 is added to account for padding or unknown tokens (if any).\n",
        "\n",
        "4. This ensures that the one-hot encoded vectors have the correct dimensionality, matching the number of classes (words in the vocabulary)."
      ],
      "metadata": {
        "id": "7hNQ-fN8xRMO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_labels(y,tokenizer):\n",
        "  #Applying categorical transformation to make it a multiclass classification problem\n",
        "  return to_categorical(y,num_classes=len(tokenizer.word_index)+1)"
      ],
      "metadata": {
        "id": "IlwgueTdmEu1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train=preprocess_labels(y_train,tokenizer)\n",
        "y_val=preprocess_labels(y_val,tokenizer)"
      ],
      "metadata": {
        "id": "rnUSYGk-w39-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model1 Structure:\n",
        "1. Embedding Layer\n",
        "2. Bidirectional LSTM layer\n",
        "3. Bidirectional LSTM layer\n",
        "4. Dense Layer"
      ],
      "metadata": {
        "id": "0zJhTatOmWXW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,Bidirectional,LSTM,Dense"
      ],
      "metadata": {
        "id": "O6suD0r8mP20"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1=Sequential()\n",
        "model1.add(Embedding(input_dim=len(tokenizer.word_index)+1,output_dim=200))\n",
        "'''\n",
        "output_dim: This is the size of the word vectors (embeddings). You're setting it to 200, meaning each word will be represented by a 200-dimensional vector.\n",
        "This layer converts each word index (from the tokenizer) into a dense embedding vector\n",
        "'''\n",
        "model1.add(Bidirectional(LSTM(256,return_sequences=True)))\n",
        "model1.add(Bidirectional(LSTM(256)))\n",
        "model1.add(Dense(len(tokenizer.word_index)+1,activation='softmax'))"
      ],
      "metadata": {
        "id": "bTiJuyYOm0mR"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary of the Model\n",
        "Input: Integer-encoded sequences of words (from the tokenizer).\n",
        "\n",
        "Embedding Layer: Converts words into dense 200-dimensional vectors.\n",
        "\n",
        "Bidirectional LSTMs: Two layers of bidirectional LSTMs process the sequence to capture contextual information.\n",
        "\n",
        "Output Layer: A dense layer with softmax activation predicts the next word (or class) based on the processed sequence."
      ],
      "metadata": {
        "id": "z54-juV5oKje"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model1.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "9oFwgjlWncuu"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model1.fit(X_train,y_train,epochs=10,batch_size=128,validation_data=(X_val,y_val))"
      ],
      "metadata": {
        "id": "YzPsKXCux9P-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}